{
    "run_name": "GQA and MoE on TinyStories",
    "seed": 42,
    "dataset": "TinyStories",
    "data_dir": "/home/kemove/Courses/STF_LLM/Assignment_1/data/TinyStories",
    "train_file": "ts2_train.txt",
    "valid_file": "ts2_valid.txt",
    "vocab_file": "/home/kemove/Courses/STF_LLM/Assignment_1/data/TinyStories/vocab.json",
    "merges_file": "/home/kemove/Courses/STF_LLM/Assignment_1/data/TinyStories/merges.txt",
    "special_tokens": [
        "<|endoftext|>"
    ],
    "attention_type": "GQA",
    "context_length": 512,
    "d_model": 512,
    "num_layers": 8,
    "num_heads": 16,
    "d_ff": 1344,
    "rope_theta": 10000.0,
    "dropout": 0.1,
    "use_moe": true,
    "moe_layers": [
        1,
        2,
        3,
        4,
        5,
        6,
        7
    ],
    "n_routed_experts": 4,
    "num_experts_per_tok": 1,
    "n_shared_experts": 1,
    "aux_seq_loss_alpha": 0.01,
    "bias_update_speed": 0.01,
    "num_kv_heads": 4,
    "batch_size": 128,
    "max_iterations": 10000,
    "max_lr": 0.0005,
    "min_lr": 5e-05,
    "warmup_iterations": 500,
    "beta1": 0.9,
    "beta2": 0.999,
    "eps": 1e-08,
    "weight_decay": 0.1,
    "max_grad_norm": 1.0,
    "log_interval": 50,
    "eval_interval": 500,
    "eval_batches": 100,
    "checkpoint_dir": "/home/kemove/Courses/STF_LLM/Assignment_1/checkpoints",
    "use_amp": true
}