{
    "checkpoint_path": "/home/kemove/Courses/STF_LLM/Assignment_1/checkpoints/TinyStories/best_model.pt",
    "vocab_file": "/home/kemove/Courses/STF_LLM/Assignment_1/data/TinyStories/vocab.json",
    "merges_file": "/home/kemove/Courses/STF_LLM/Assignment_1/data/TinyStories/merges.txt",
    "special_tokens": [
        "<|endoftext|>"
    ],
    "attention_type": "MHA",
    "context_length": 512,
    "d_model": 512,
    "num_layers": 8,
    "num_heads": 16,
    "d_ff": 1344,
    "rope_theta": 10000.0,
    "dropout": 0.0,
    "use_moe": true,
    "moe_layers": [
        1,
        2,
        3,
        4,
        5,
        6,
        7
    ],
    "n_routed_experts": 4,
    "num_experts_per_tok": 1,
    "n_shared_experts": 1,
    "aux_loss_alpha": 0.01,
    "num_kv_heads": 16,
    "max_new_tokens": 200,
    "temperature": 0.8,
    "top_k": 0,
    "top_p": 0.9,
    "use_amp": true
}